{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Aprendizado Nao Supervisionado.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2-ELSZ2gbffT"},"source":["# <center>Métodos de Aprendizado não Supervisionado</center>"]},{"cell_type":"markdown","metadata":{"id":"VuX0sQnnbffX"},"source":["<a id=\"recap\"></a>\n","## 1. Introdução"]},{"cell_type":"markdown","metadata":{"id":"dD__jDwnbffY"},"source":["Nos capítulos anteriores, exploramos conceitos iniciais de ML, numpy/pandas e uma introdução a algumas das técnicas de EDA. Neste capítulo, vamos nos concentrar em uma abordagem diferente de ML: a **Aprendizagem não Supervisionada**. Mais especificamente, vamos nos aprofundar nas principais técnicas e algoritmos utilizados para abordar este tópico, explorando as armadilhas mais comuns que esse tipo de problema traz, como implementar esse tipo de algoritmo usando Python e como avaliar e selecionar o melhor modelo para seu problema.\n","\n","> **Recapitulação da definição**: a principal diferença entre os tipos não supervisionado e supervisionado é que o **Aprendizado supervisionado** é feito utilizando um conhecimento prévio da variável resposta, ou em outras palavras, temos conhecimento prévio de quais devem ser os valores de saída de nossos modelos. Portanto, o objetivo da aprendizagem supervisionada é aprender uma função que, dada uma amostra de dados e saídas desejadas, melhor as correlacione. A **Aprendizagem não Supervisionada**, por outro lado, não possui saídas rotuladas, então seu objetivo é inferir a estrutura natural presente dentro de um conjunto de dados. As tarefas mais comuns na aprendizagem não supervisionada são a clusterização e a análise de associação. O aprendizado não supervisionado também é muito utilizado na análise exploratória já que é capaz de identificar agrupamentos ou similaridade entre as instâncias analisadas.\n","\n","Para apresentar as técnicas de **Aprendizagem não Supervisionada** mais comuns, esta aula será dividida em 3 seções. A primeira apresenta a metodologia de Análise de Associação, útil para descobrir correleações ocultas em grandes conjuntos de dados. A segunda seção apresenta a Análise de Cluster, um grupo de técnicas que o ajudará a descobrir semelhanças entre instâncias. Por fim, teremos uma última seção que tratara sobre a técnica de Soft clustering.\n"]},{"cell_type":"markdown","metadata":{"id":"x-aoAT3qbffZ"},"source":["<a id=\"association_analysis\"></a>\n","## 2. Análise de Associação"]},{"cell_type":"markdown","metadata":{"id":"4D0zaRqJbffZ"},"source":["<a id=\"problem_definition_association\"></a>\n","### 2.1. Definição do Problema\n","\n","Imagine a seguinte situação hipotética: você possui uma loja de varejo que vende produtos ao público em quantidades relativamente pequenas e percebeu que quase todos os clientes que compram fraldas também compram cervejas. Naturalmente, você se pergunta: _ \"Nossa, que padrão estranho! Será que devo colocar os dois produtos lado a lado na prateleira ?\" _. Bem, é um tipo de correlação estranha, mas imagine que você pudesse identificar padrões comuns em todos os itens vendidos por sua loja. Não seria interessante ?!\n","\n","Infelizmente, esta história é provavelmente uma lenda urbana de dados. No entanto, é um exemplo ilustrativo (e divertido) dos insights que podem ser obtidos pela **Análise de associação**, que tenta encontrar padrões comuns sobre itens em grandes conjuntos de dados. Esta aplicação específica é frequentemente chamada de análise de cesta de compras (mais especificamente, este é o caso do \"cerveja e fraldas\"), mas também pode ser aplicada a outras situações, pedido de peças de reposição e mecanismos de recomendação online - apenas para citar um pouco.\n","\n","Para apresentá-lo ao aprendizado de regras de associação, vamos examinar o Dataset chamado **Online Retail Data Set**, que contém todas as transações ocorridas entre 01/12/2010 e 09/12/2011 para um e-commerce."]},{"cell_type":"code","metadata":{"id":"CBQYKn9jbffa","scrolled":true},"source":["# Leitura do Dataset\n","import pandas as pd\n","\n","df = pd.read_excel('Online_Retail.xlsx')\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7EZgByxXbffe"},"source":["# É necessária uma primeira etapa de tratamento dos dados.\n","# Primeiramente, algumas das descrições possuem espaços que precisam ser removidos. \n","# Além disso, também iremos remover linhas sem informação\n","\n","df['Description'] = df['Description'].str.strip()\n","df.dropna(axis=0, subset=['InvoiceNo'], inplace=True)\n","df['InvoiceNo'] = df['InvoiceNo'].astype('str')\n","df = df[~df['InvoiceNo'].str.contains('C')]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xrM8cET-bffi"},"source":["<a id=\"initial_analysis_association\"></a>\n","### 2.2. Análises Iniciais\n","\n","A Análise de Associação é relativamente simples em termos matemáticos. Esta técnica é um bom começo para certos casos de exploração de dados e pode apontar o caminho para uma análise mais profunda nos dados utilizando outras abordagens.\n","\n","Mas antes de começarmos a modelar nosso problema, há alguns termos usados na análise de associação que são fundamentais para sua compreenção: ***Itemset, Suporte, Confiança e Lift***. Nas próximas subseções, explicamos esses termos em detalhes, com base nas seguintes transações (Exemplificadas na imagem abaixo):\n","\n","<img src=\"https://annalyzin.files.wordpress.com/2016/04/association-rule-support-table.png?w=376&h=334\" width=\"350\">\n","\n","\n","#### 2.2.1 Itemset\n","A Análise de Associação tenta identificar associações frequentes \"se-então\" chamadas regras de associação, que consistem em um antecedente (se) e um consequente (então). Para uma determinada regra, chamamos ***Itemset*** a lista de todos os itens no antecedente e no consequente. Por exemplo: “Se maçã e cerveja, arroz” (“Se maçã e cerveja forem comprados, então há uma grande chance de que arroz também seja comprado pelo cliente”). Nesse caso, maçã e cerveja são o antecedente e o arroz é o consequente.\n","\n","#### 2.2.2 Suporte\n","\n","Isso mostra a popularidade de um conjunto de itens, medido pela proporção de transações nas quais um conjunto de itens aparece. Na tabela mostrada acima, o suporte de {Maçã} é 4 de 8, ou 50%. Os conjuntos de itens também podem conter vários itens. Por exemplo, o suporte de {maçã, cerveja, arroz} é 2 de 8, ou 25%.\n","\n","![img](https://annalyzin.files.wordpress.com/2016/03/association-rule-support-eqn.png?w=186&h=51)\n","\n","Se você descobrir que as vendas de itens além de uma determinada proporção tendem a ter um impacto significativo em seus lucros, você pode considerar usar essa proporção como seu limite de suporte.\n","\n","#### 2.2.3 Confiança\n","\n","Isso indica a probabilidade de compra do item Y quando o item X é comprado, expressa como {X -> Y}. Isso é medido pela proporção de transações com o item X, em que o item Y também aparece. Na Tabela 1, a confiança de {Maçã -> Cerveja} é 3 de 4, ou 75%.\n","\n","![img](https://annalyzin.files.wordpress.com/2016/03/association-rule-confidence-eqn.png?w=394&h=57)\n","\n","Uma desvantagem da medida de confiança é que ela pode representar mal a importância de uma associação. Isso ocorre porque ela só explica a popularidade das maçãs, mas não das cervejas. Se as cervejas também forem muito populares em geral, haverá uma chance maior de que uma transação contendo maçãs também contenha cervejas, aumentando assim a medida de confiança.\n","\n","#### 2.2.4 Lift\n","\n","Isso diz a probabilidade de o item Y ser comprado quando o item X é comprado, enquanto controla a popularidade do item Y. Na tabela mostrada acima, o Lift de {maçã -> cerveja} é 1, o que não implica nenhuma associação entre os itens. Um valor de Lift maior que 1 significa que o item Y provavelmente será comprado se o item X for comprado, enquanto um valor inferior a 1 significa que o item Y provavelmente não será comprado se o item X for comprado.\n","\n","![img](https://annalyzin.files.wordpress.com/2016/03/association-rule-lift-eqn.png?w=424&h=60)\n","\n","<a id=\"modeling_association\"></a>\n","### 2.3. Modelagem\n","\n","#### 2.3.1 Apriori\n","\n","Apriori é um algoritmo popular para extrair conjuntos de itens frequentes com aplicações no aprendizado de regras de associação. Para grandes conjuntos de dados, pode haver centenas de itens em centenas de milhares de transações. O algoritmo a priori tenta extrair regras para cada combinação possível de itens. Por exemplo, o Lift pode ser calculado para o item 1 e item 2, item 1 e item 3, item 1 e item 4 e, em seguida, item 2 e item 3, item 2 e item 4 e, em seguida, combinações de itens, por exemplo item 1, item 2 e item 3; da mesma forma, item 1, item2 e item 4 e assim por diante.\n","\n","O algoritmo a priori foi projetado para operar em bancos de dados contendo transações, como compras de clientes de uma loja. Um conjunto de itens é considerado \"frequente\" se atender a um limite de suporte especificado pelo usuário. Por exemplo, se o limite de suporte for definido como 0,5 (50%), um conjunto de itens frequente é definido como um conjunto de itens que ocorrem juntos em pelo menos 50% de todas as transações no banco de dados.\n","\n","Para aplicar o algoritmo Apriori, utilizaremos a implementação python em [MLxtend](http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/)."]},{"cell_type":"code","metadata":{"id":"U2tHWDtCbffk"},"source":["# Caso não possua, você deve instalar a biblioteca mlxtend\n","from mlxtend.frequent_patterns import apriori\n","from mlxtend.frequent_patterns import association_rules"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cue4Blllbffq"},"source":["A função implementada de MLxtend espera dados em um DataFrame pandas codificado no formato one-hot encoding. Isso significa que os itens de dados devem ser consolidados em uma transação por linha. Isso pode ser feito manualmente como ilustrado abaixo."]},{"cell_type":"code","metadata":{"id":"2ONzwfRhbffr"},"source":["# Consolide os itens em 1 transação por linha.\n","# Para manter o conjunto de dados pequeno, analisaremos apenas as vendas para a França.\n","# obs.: uma outra maneira de fazer isto é usando o método pivot_table()\n","basket = (df[df['Country'] ==\"France\"]\n","          .groupby(['InvoiceNo', 'Description'])['Quantity']\n","          .sum().unstack().reset_index().fillna(0)\n","          .set_index('InvoiceNo'))\n","basket.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"keBg14Ssbffu"},"source":["Além disso, o algoritmo apriori só aceita números inteiros. Precisamos substituir todos os valores ≥1 por 1 e <1 por 0."]},{"cell_type":"code","metadata":{"id":"k_cez62ubffu"},"source":["# Certifique-se de que todos os valores positivos sejam convertidos em 1 e qualquer valor menor que 0 seja definido como 0\n","def encode_units(x):\n","    if x <= 0:\n","        return 0\n","    if x >= 1:\n","        return 1\n","\n","basket_sets = basket.applymap(encode_units)\n","basket_sets.drop('POSTAGE', inplace=True, axis=1)\n","basket_sets.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BMOJcyVGbffy"},"source":["Agora que os dados estão estruturados corretamente, podemos gerar conjuntos de itens frequentes que têm um suporte de pelo menos 7% (esse número foi escolhido arbitrariamente)."]},{"cell_type":"code","metadata":{"id":"8Gg0Aq03bff2"},"source":["frequent_itemsets = apriori(basket_sets, min_support=0.07, use_colnames=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pp7ZZNb2bff7"},"source":["Finalmente, podemos gerar as regras com seu suporte, confiança e lift correspondentes:"]},{"cell_type":"code","metadata":{"id":"bVFWCQ-Obff8","scrolled":true},"source":["rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n","rules"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mNn07Kk_bfgB"},"source":["Bem, isso é tudo que há para fazer! Acabamos de construir os itens frequentes usando apriori e, em seguida, construir as regras com association_rules. Mas agora, a parte complicada é descobrir o que isso nos diz. Por exemplo, podemos ver que existem algumas regras com um alto Lift, o que significa que ocorre com mais frequência do que seria esperado, dado o número de combinações de transações e produtos.\n","\n","### Exercicio 1\n","\n","Use a célula abaixo para verificar as regras com aumento acima de 6 e confiança acima de 0,6. Que conclusões você consegue obter? Discuta"]},{"cell_type":"code","metadata":{"id":"O_IePJPubfgB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T4kWt5gjbfgG"},"source":["Além disso, não seria interessante ver como as combinações variam de acordo com o país de compra? Use a célula abaixo para verificar algumas combinações populares na Alemanha. Você consegue obter algum insight ?"]},{"cell_type":"code","metadata":{"id":"UUUTdcOQbfgH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZeIMpS85bfgL"},"source":["<a id=\"clustering_analysis\"></a>\n","## 3. Análise de Clusterização"]},{"cell_type":"markdown","metadata":{"id":"WVwo7o4dbfgM"},"source":["Na seção anterior, apresentamos a metodologia de Análise de Associação, que é um dos métodos de Aprendizagem não Supervisionados mais comuns. Agora vamos apresentar a você outra técnica extremamente usada: a **Análise de Clusterização**."]},{"cell_type":"markdown","metadata":{"id":"erCN2_ngbfgM"},"source":["<a id=\"problem_definition_clustering\"></a>\n","### 3.1. Definição do Problema\n","\n","Suponha que você esteja agora na seguinte situação hipotética: você ainda possui uma loja de varejo e depois de identificar padrões comuns de itens vendidos por sua loja, agora você gostaria de atingir grupos específicos de clientes com campanhas publicitárias específicas.\n","\n","Para fazer isso, você se pergunta: _\"Existe alguma maneira de identificar quais são os diferentes tipos de clientes que compram em minha loja, por exemplo, considerando as características de meus clientes, como histórico de compras, interesses ou monitoramento de atividades padrões?\"_. Bem, neste caso, a Analise de Clusterização poderia definitivamente ajudá-lo a responder a esta pergunta.\n","\n","![img](https://pro.arcgis.com/en/pro-app/tool-reference/spatial-statistics/GUID-A06A412D-2F4F-4D35-8FFF-1F4B3B3A8F16-web.png)\n","\n","De maneira simplificada, o objetivo da Clusterização é encontrar grupos diferentes dentro dos dados. Para fazer isso, os algoritmos de clusterização encontram a estrutura nos dados de forma que os elementos do mesmo cluster (ou grupo) sejam mais semelhantes uns aos outros do que aos de diferentes clusters.\n","\n","Dado um conjunto de pontos de dados, podemos usar um algoritmo de agrupamento para classificar cada ponto de dados em um grupo específico. Em teoria, os pontos de dados que estão no mesmo grupo devem ter propriedades e/ou features semelhantes, enquanto os pontos de dados em grupos diferentes devem ter propriedades e/ou features consideravelmente diferentes.\n","\n","Nas próximas subseções, discutiremos e implementaremos alguns algoritmos de agrupamento. No entanto, diferentemente do que fizemos na parte de Análise de associação, não trabalharemos em um único conjunto de dados. Em vez disso, para cada algoritmo apresentado, trabalharemos em um conjunto de dados específico."]},{"cell_type":"markdown","metadata":{"id":"cy8Si84obfgN"},"source":["<a id=\"initial_analysis_clustering\"></a>\n","### 3.2. Análises Iniciais\n","\n","A Clusterização é utilizada para determinar o agrupamento intrínseco entre os dados não rotulados presentes. No entanto, \"não há critérios\" claros para analisar um bom agrupamento. Naturalmente, pode-se utilizar critérios (como veremos a frente) relacionados a inércia dos clusters, porém, isso não avalia se, de fato, conseguimos fazer separações que façam sentido do ponto de vista prático ou de negócios. Portanto, cabe ao usuário determinar quais são os critérios que ele pode usar para atender às suas necessidades. \n","Por exemplo, podemos estar interessados em encontrar representantes para grupos homogêneos (data reduction), em encontrar \"clusters naturais\" e descrever suas propriedades desconhecidas, em encontrar agrupamentos úteis e adequados ou na localização de amostras incomuns (detecção de outlier). Este algoritmo deve fazer algumas suposições que constituem a similaridade de pontos e cada suposição faz clusters diferentes e igualmente válidos.\n","\n","Antes de começarmos a modelar, há dois conceitos importantes que precisamos abordar. Estamos falando sobre ***Determinar o número de clusters*** (que deve ser feito antes de rodar o algoritmo) e ***Feature Selection*** (ou seleção de features/variáveis).\n","\n","#### 3.2.1 Determinando o número de clusters\n","\n","Determinar o número ideal de clusters em um conjunto de dados é uma questão fundamental no processo. Infelizmente, não há uma resposta definitiva para essa pergunta. O número ideal de clusters é de alguma forma subjetiva e depende do método usado para medir semelhanças e dos parâmetros usados para particionamento. Por exemplo, se você deseja segmentar clientes que têm maior probabilidade de comprar cervejas e clientes que provavelmente não compram cervejas, pode definir um número de dois grupos. Ou, ainda, o número de clusters pode ser definido anteriormente através de regras ou restrições de negócio.\n","\n","No entanto, se você não tem ideia de quantos clusters precisa, pode usar alguns métodos para determinar o número ótimo.\n","\n","#### 3.2.2 Feature Selection\n","\n","Consiste em criar um subconjunto de uma lista de features/variáveis úteis entre todo o conjunto de variáveis à nossa disposição. Esta etapa pode parecer contra-intuitiva, uma vez que estamos excluindo informações que nosso modelo futuro poderia aprender, mas, se feito da maneira certa, a seleção de features pode até ser capaz de melhorar o desempenho do modelo. \n","\n","Um dos métodos estatísticos mais comuns utilizados para lidar com feature selection é o que chamamos de ***Análise de Componentes Principais (PCA)***. Imagine que a dimensionalidade do conjunto de features é maior do que apenas dois ou três. Usando o PCA, podemos agora identificar quais são as dimensões mais importantes e apenas manter algumas delas para explicar a maior parte da variação que vemos em nossos dados.\n","\n","Além disso, o PCA pode ser realmente útil para visualização e compressão de dados. Os dados nem sempre vêm com dimensionalidade igual ou menor a 3 (ou seja, 3 variáveis / features). Portanto, não podemos conceber uma visualização do gráfico de dispersão de nossos dados, uma vez que estamos limitados a apenas 3 dimensões. Isso torna impossível para nós ver sua distribuição conjunta neste espaço N-dimensional. Mas, usando o PCA, podemos contornar esse problema retendo apenas as dimensões mais úteis (ou seja, aquelas que explicam a maior parte da variação que vemos em nossos dados). No entanto, essas dimensões não correspondem às nossas originais. O PCA tenta encontrar um sistema de coordenadas neste espaço N-dimensional que maximize a variância ao longo de cada eixo.\n","\n","Não entraremos em mais detalhes, mas o PCA também é uma técnica de aprendizado não supervisionado. Fica a sugestão de pesquisa!"]},{"cell_type":"markdown","metadata":{"id":"_I5SS9PObfgN"},"source":["<a id=\"modeling_clustering\"></a>\n","### 3.3. Modelagem"]},{"cell_type":"markdown","metadata":{"id":"A48emC9bbfgO"},"source":["### Kmeans"]},{"cell_type":"markdown","metadata":{"id":"WuAH3bP3bfgO"},"source":["O algoritmo K-means foi proposto como uma forma de **agrupar pontos de dados semelhantes em clusters**. Como veremos a frente, o algoritmo k-means é extremamente fácil de implementar e também é computacionalmente muito eficiente em comparação com outros algoritmos de agrupamento, o que pode explicar sua popularidade.\n","\n","Este algoritmo pertence à categoria de **prototype-based clustering**. Isso significa que cada cluster é representado por um protótipo, que pode ser o **centróide (média)** de pontos semelhantes. Embora k-means seja muito bom para identificar grupos de forma esférica, uma das desvantagens deste algoritmo de agrupamento é que temos que especificar o número de clusters k a priori.\n","\n","Uma escolha inadequada para k pode resultar em agrupamento de mal desempenho. Além disso, discutiremos o **método do cotovelo e a silhueta**, que são técnicas úteis para avaliar a qualidade de um agrupamento para nos ajudar determinar o número ideal de clusters k.\n"]},{"cell_type":"markdown","metadata":{"id":"Zut4Be4ZbfgP"},"source":["#### Algoritmo"]},{"cell_type":"markdown","metadata":{"id":"_THljbN2bfgQ"},"source":["O algoritmo k-means pode ser resumido pelas quatro etapas a seguir:\n","\n","1. Escolha aleatoriamente k centróides dos pontos da amostra como centros iniciais do cluster.\n","2. Atribua cada amostra ao centroide mais próximo\n","3. Mova os centróides para o centro das amostras que foram atribuídas a ele.\n","4. Repita as etapas 2 e 3 até que as atribuições do cluster não mudem ou uma tolerância definida pelo usuário ou um número máximo de iterações seja alcançado.\n","\n","![img](https://thumbs.gfycat.com/InbornCloseFlickertailsquirrel-small.gif)"]},{"cell_type":"markdown","metadata":{"id":"8pXGD8tabfgR"},"source":["Podemos definir semelhança como o oposto de distância. Uma fórmula comumente utilizada para avaliar distância em agrupamento de\n","amostras com features contínuas é a distância euclidiana entre dois pontos x e y em um espaço m-dimensional:"]},{"cell_type":"markdown","metadata":{"id":"8cZHjLm-bfgS"},"source":["\\begin{equation*}\n","d(u,v) = \\sqrt{\\sum_{j=1}^{m}(u_{j} - v_{j})^2} = \\left \\|u_{j} - v_{j}  \\right \\|\n","\\end{equation*}"]},{"cell_type":"markdown","metadata":{"id":"yX6Je1pbbfgT"},"source":["Observe que, na equação anterior, o índice j se refere à j-ésima dimensão (coluna de característica) dos pontos de amostra u e v."]},{"cell_type":"markdown","metadata":{"id":"hEGSOQ7dbfgV"},"source":["Com base nesta métrica de distância euclidiana, podemos descrever o algoritmo de k-means como um problema de otimização simples: uma abordagem iterativa para minimizar **a soma dos erros quadráticos (SSE) dentro do cluster**, que às vezes também é chamada de **inércia do cluster**:"]},{"cell_type":"markdown","metadata":{"id":"R9l1ACg5bfgW"},"source":["\\begin{equation*}\n","SSE = \\sum_{i=1}^{n} \\sum_{j=1}^{k} w_{(i,j)} \\left \\|x_{i} - \\mu_{j}  \\right \\|\n","\\end{equation*}"]},{"cell_type":"markdown","metadata":{"id":"WqAsG4ulbfgX"},"source":["Aqui, $\\mu_ {j}$ é o ponto representativo (centróide) para o cluster j, <br>\n","$w_ {i, j}=1$ se a amostra $x_ {i}$ está no cluster j, $w_ {i, j} = 0$ caso contrário"]},{"cell_type":"markdown","metadata":{"id":"taUxf-9VbfgZ"},"source":["##### Exemplo"]},{"cell_type":"markdown","metadata":{"id":"IfXQjdaebfga"},"source":["Por questão de simplicidade, vamos criar um conjunto de dados de cluster bem definido, usando o método ```blob``` da biblioteca sklearn."]},{"cell_type":"code","metadata":{"id":"aCRUA6tSbfgc"},"source":["%matplotlib inline\n","# Importando make blobs e matplotlib\n","from sklearn.datasets import make_blobs\n","import matplotlib.pyplot as plt\n","\n","# Criando blobs (dados aleatórios em torno de centros definidos)\n","X,y = make_blobs(n_samples=150,\n","                 n_features=2,\n","                 centers=3,\n","                 cluster_std=0.4,\n","                 shuffle=True,\n","                 random_state=0)\n","\n","# Plotando os blobs\n","plt.scatter(X[:,0],\n","            X[:,1],\n","            c='black',\n","            marker='o',\n","            s=50)\n","\n","plt.grid()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vf5is50Gbfgf"},"source":["Podemos usar KMeans do sklearn para realizar clusterização entre nosso conjunto de dados construído"]},{"cell_type":"code","metadata":{"id":"8vI2QmCDbfgf"},"source":["# Importando o KMeans\n","from sklearn.cluster import KMeans\n","\n","# Criando o objeto KMeans \n","# Observe que já sabemos a priori quantos clusters precisaremos\n","num_clusters = 3\n","km = KMeans(n_clusters=num_clusters)\n","\n","# Performando clusterização K-means\n","cluster_km = km.fit_predict(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v5qRWpwpbfgj"},"source":["# Plotando os dados com cores para o cluster\n","for cluster in range(num_clusters):\n","        plt.scatter(X[cluster_km==cluster,0],\n","                    X[cluster_km==cluster,1],\n","                    s=50,\n","                    cmap='Pastel1',\n","                    label='cluster {}'.format(cluster))\n","\n","# Plotando o centro dos CLusters\n","plt.scatter(km.cluster_centers_[:,0],\n","            km.cluster_centers_[:,1],\n","            s=250,\n","            c='black',\n","            marker='*',\n","            label='centroid')\n","\n","plt.legend()\n","plt.grid()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RqWZkRvdbfgq"},"source":["Embora k-means funcione bem neste conjunto de dados, precisamos ressaltar alguns dos\n","principais desafios do k-means. Uma das desvantagens do k-means é que temos que\n","especificar o número de clusters k a priori, o que pode nem sempre ser tão óbvio em\n","aplicações do mundo real, especialmente se estivermos trabalhando com uma dimensão mais elevada do\n","conjunto de dados que não pode ser visualizado. As outras propriedades do k-means são que os clusters\n","não se sobrepõem e não são hierárquicos, e também assumimos que há pelo menos\n","um item em cada cluster."]},{"cell_type":"markdown","metadata":{"id":"yhI8Fgqobfgr"},"source":["#### Métodos de Validação - Curva do cotovelo"]},{"cell_type":"markdown","metadata":{"id":"tFungrSmbfgr"},"source":["A fim de quantificar a **qualidade da clusterização**, precisamos usar métricas intrínsecas, como o SSE dentro do cluster (distorção) que discutimos anteriormente neste capítulo - para comparar o desempenho de diferentes agrupamentos k-means.\n","Convenientemente, não precisamos calcular o SSE dentro do cluster explicitamente, pois é já acessível através do atributo ``inertia`` após fazer o fit de um modelo KMeans:"]},{"cell_type":"code","metadata":{"id":"wkjgxe2Dbfgs"},"source":["print('Distorção: %.2f' % km.inertia_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"celpx3CZbfgv"},"source":["Com base no SSE dentro do cluster, podemos usar uma ferramenta gráfica, o chamado método do cotovelo, para estimar o número ótimo de clusters k para uma determinada tarefa. Intuitivamente,\n","podemos dizer que, se k aumentar, a distorção diminuirá. Isso ocorre porque as amostras estarão mais próximas dos centróides aos quais estão atribuídas. A ideia por trás do\n","método do cotovelo é identificar o valor de k onde a distorção começa a aumentar\n","mais rapidamente, o que ficará mais claro se traçarmos a distorção para diferentes\n","valores de k:"]},{"cell_type":"code","metadata":{"id":"QfnZjRqHbfgw"},"source":["# Criando lista vazia\n","distortions = []\n","\n","# Cálculo da distorção para uma série de valores de k\n","for i in range(1, 11):\n","    km = KMeans(n_clusters=i)\n","    km.fit(X)\n","    distortions.append(km.inertia_)\n","    \n","# Gráfico da Distorção\n","plt.figure(figsize=(8,5))\n","plt.plot(range(1,11), distortions, marker='o')\n","plt.xticks(range(1,11))\n","plt.xlabel('Número de Clusters')\n","plt.ylabel('Distorção')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4RWgAe2vbfgy"},"source":["Como podemos ver no gráfico a seguir, o cotovelo está localizado em k = 3, o que fornece\n","evidência de que k = 3 é de fato uma boa escolha para este conjunto de dados."]},{"cell_type":"markdown","metadata":{"id":"JYHj_dhPbfgz"},"source":["#### Coeficiente de Silhueta"]},{"cell_type":"markdown","metadata":{"id":"wASoTC9Dbfgz"},"source":["A análise da silhueta pode ser usada para medir a coesão do cluster entre os pontos de dados e o centróide. Para o cálculo, tem-se o passo a passo:\n","1. Calcule a coesão do cluster $a_ {i}$ como a distância média entre uma amostra $x_ {i}$ e todos os outros pontos no mesmo cluster.\n","2. Calcule a separação de cluster $b_ {i}$ do próximo cluster mais próximo como a distância média entre a amostra $x_ {i}$ e todas as amostras no cluster mais próximo.\n","3. Calcule a silhueta $s_ {i}$ como a diferença entre a coesão e a separação do cluster dividida pelo maior dos dois, conforme mostrado aqui:\n","\n"," \\begin{equation*}\n","s_{i} = \\frac{b_{i} - a_{i}}{max\\left \\{ b_{i},a_{i} \\right \\}}\n","\\end{equation*}"]},{"cell_type":"markdown","metadata":{"id":"vx10R2wwbfg0"},"source":["O coeficiente de silhueta é limitado no intervalo de -1 a 1. Com base na fórmula anterior, podemos ver que o coeficiente de silhueta é 0 se a separação e coesão do cluster forem iguais ($ b_ {i} = a_ {i} $). Além disso, chegamos perto de um coeficiente de silhueta ideal de 1 se ($ b_ {i} >> a_ {i} $), uma vez que $ b_ {i} $ quantifica quão diferente é uma amostra de outros clusters, e $ a_ {i } $ nos diz o quão semelhante são as outras amostras em seu próprio cluster."]},{"cell_type":"markdown","metadata":{"id":"bQv8ZB5Cbfg0"},"source":["O coeficiente de silhueta está disponível como ```silhouette_score``` no módulo ```sklearn.metrics```. Isso calcula o coeficiente de silhueta médio em todas as amostras, que é equivalente a numpy.mean (silhouette_samples (…))."]},{"cell_type":"markdown","metadata":{"id":"9AnhF5-Fbfg2"},"source":["Para melhor ilustrar esta importante métrica, você pode executar as 2 células a seguir e alterar a dispersão dos dados no widget ```cluster_cohesion```. Não se preocupe se você não entender parte do código que usamos aqui para fazer o plot."]},{"cell_type":"code","metadata":{"id":"fCOlpyR1bfg4"},"source":["# Importando Python widgets para construit o plot interativo\n","# Execute o comando comentado abaixo, caso tenha problema no import\n","# !jupyter nbextension enable --py widgetsnbextension\n","\n","import ipywidgets as widgets\n","from ipywidgets import interact, interact_manual"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xC6l52O0bfg8","scrolled":false},"source":["# Importando silhouette_score\n","from sklearn.metrics import silhouette_score\n","\n","# Declarando esta função para ser iteraiva\n","@interact\n","def calculate_kmeans(Dispersao=(0.1,0.8,0.1), n_cluster=(2,5,1)):\n","    # Criando os blobs\n","    X,y = make_blobs(n_samples=500,\n","                 n_features=2,\n","                 centers=3,\n","                 cluster_std=Dispersao,\n","                 shuffle=True,\n","                 random_state=0)\n","\n","    km = KMeans(n_clusters=n_cluster)\n","    \n","    # Clusterização K-Means \n","    cluster_km = km.fit_predict(X)\n","    score = silhouette_score(X,\n","                             km.labels_,\n","                             metric='euclidean')\n","    \n","    print(\"SSE: {}, Coeficiente de Silhueta: {}\".format(km.inertia_,score))\n","    \n","    # Plotando os CLusters\n","    plt.figure(figsize = (8,5))\n","    for cluster in range(n_cluster):\n","        plt.scatter(X[cluster_km==cluster,0],\n","                    X[cluster_km==cluster,1],\n","                    s=50,\n","                    cmap='Pastel1',\n","                    marker='s',\n","                    label='cluster {}'.format(cluster))\n","    \n","    # Plotando o centro dos clusters\n","    plt.scatter(km.cluster_centers_[:,0],\n","                km.cluster_centers_[:,1],\n","                s=250,\n","                c='black',\n","                marker='*',\n","                label='centroid')\n","    \n","    \n","    plt.legend()\n","    plt.grid()\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WGiEOX2Kbfg_"},"source":["Observe que, quando a dispersão dos dados é pequena (clusters são mais coesos), a pontuação da silhueta fica mais próxima de 1. <br>\n","E, mesmo se a dispersão for 0.1, mas o número do cluster não for 3, a pontuação sai de 1. <br>\n","\n","Tente variar o número de clusters para cima e para baixo no widget e veja o que ocorre (não há a necessidade de alterar o código).\n","<br>\n","<br>\n","Portanto, este experimento mostra a importância de equilibrar o número correto de clusters usando algumas métricas de avaliação e como essa escolha afeta o desempenho do modelo."]},{"cell_type":"markdown","metadata":{"id":"jqhf7_YHbfg_"},"source":["### Exercicio 2\n","\n","Agora que você aprendeu os conceitos de agrupamento Kmeans, vamos tentar usá-lo com nosso conjunto de dados de varejo para agrupar nossos clientes. Primeiro, precisamos tratar um pouco os dados e criar algumas features relacionadas ao cliente para que possamos utilizá-las com o algoritmo. Como não é o foco desta lição, não comentaremos muito sobre esta primeira parte."]},{"cell_type":"code","metadata":{"id":"3Urc6Nn3bfhA"},"source":["# Criando preço por produto\n","df['Price'] = df.Quantity * df.UnitPrice\n","\n","# Consolidando os itens em 1 cliente por linha.\n","product_features = (df.groupby(['CustomerID', 'Description'])['Quantity']\n","                    .sum().unstack().reset_index().fillna(0)\n","                    .set_index('CustomerID'))\n","\n","# Certifique-se de que todos os valores positivos sejam convertidos em 1 e qualquer valor menor que 0 seja definido como 0\n","product_presence = product_features.applymap(encode_units)\n","\n","# Criando uma coluna para quantidade total e número de produtos\n","product_features[\"total\"] = product_features.sum(axis = 1, skipna = True)\n","product_presence[\"total\"] = product_presence.sum(axis = 1, skipna = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9pH5YzaGbfhF"},"source":["# Obtendo apenas os produtos mais comuns para reduzir o número de features\n","number_of_products = 10\n","most_common_products = df.Description.value_counts(sort=True)[0:number_of_products].index.tolist()\n","most_common_products.append(\"total\")\n","\n","product_features_filtered = product_features[most_common_products].add_suffix(\"_quantidade\")\n","product_presence_filtered = product_presence[most_common_products].add_suffix(\"_numero\")\n","\n","product_features_filtered.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kDc1gr7nbfhL"},"source":["# Calculando o numero de invoices por cliente\n","num_invoices = df.groupby('CustomerID').InvoiceNo.nunique().to_frame()\n","num_invoices.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gCupHnVdbfhP"},"source":["# Calculando o tempo desde o ultimo invoice\n","import numpy as np\n","last_invoice = df.groupby('CustomerID').InvoiceDate.max().to_frame()\n","last_invoice['time_from_last'] = (pd.to_datetime('today') - last_invoice['InvoiceDate']) / np.timedelta64(1,'D')\n","last_invoice.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nxQtBKeSbfhY"},"source":["# Calculando a média de preços\n","invoicePrice = df.groupby(['CustomerID', 'InvoiceNo']).Price.sum().to_frame()\n","averagePrice = invoicePrice.groupby('CustomerID').Price.mean().to_frame()\n","averagePrice.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"srnphzxDbfhb"},"source":["# Consolidando variaveis\n","final_df = pd.merge(product_features_filtered, product_presence_filtered, on = 'CustomerID')\n","final_df = pd.merge(final_df, num_invoices, on = 'CustomerID')\n","final_df = pd.merge(final_df, last_invoice.drop(columns=['InvoiceDate']), on = 'CustomerID')\n","final_df = pd.merge(final_df, averagePrice, on = 'CustomerID')\n","final_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mxpZF3Dxbfhe"},"source":["from scipy import stats\n","# Remoção de Outliers\n","final_df_no_outliers = final_df[(np.abs(stats.zscore(final_df)) < 3).all(axis=1)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ICHgVtu9bfhj"},"source":["# Scale para normalização\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Iniciando scaler\n","scaler = MinMaxScaler()\n","\n","# Aplicando scaler\n","final_df_no_outliers_scaled = pd.DataFrame(scaler.fit_transform(final_df_no_outliers))\n","\n","# Mudando nome das colunas\n","final_df_no_outliers_scaled.columns = final_df_no_outliers.columns\n","final_df_no_outliers_scaled['CustomerID'] = final_df_no_outliers.index.tolist()\n","final_df_no_outliers_scaled.set_index('CustomerID', inplace = True)\n","\n","final_df_no_outliers_scaled.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6X4jGjxYbfhn"},"source":["# Criação do Dataset final\n","customers_df = final_df_no_outliers_scaled.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"disoDiKCbfhs"},"source":["Agora que temos nosso conjunto de dados final ```customers_df``` criado, podemos plotar a distorção pelo número de clusters para escolher o melhor k."]},{"cell_type":"code","metadata":{"id":"IuVoulmpbfht"},"source":["distortions_customers = []\n","max_clusters = 50\n","\n","for i in range(1, max_clusters):\n","    km_customers = KMeans(n_clusters=i)\n","    km_customers.fit(customers_df)\n","    distortions_customers.append(km_customers.inertia_)\n","plt.figure(figsize=(8,5))\n","plt.plot(range(1,max_clusters), distortions_customers, marker='o')\n","plt.xticks(range(1,max_clusters,max_clusters//10))\n","plt.xlabel('Number of clusters')\n","plt.ylabel('Distortion')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ak9t8kQAbfhw"},"source":["Agora, usando o gráfico da célula anterior, escolha o melhor número de clusters para usar em nosso modelo final abaixo. Como agora temos um problema real, tente usar não apenas a regra do cotovelo, mas também algum conhecimento de negócios para decidir o número de clusters. Por exemplo, devemos ter 50 clusters diferentes? Isso ajudaria em nossa operação?"]},{"cell_type":"code","metadata":{"id":"INka-slTbfhx"},"source":["from sklearn.cluster import KMeans\n","\n","#Criaçao do Objeto\n","num_clusters = ___\n","km_customers = ___\n","\n","#Realizando Clusterização\n","cluster_km_customers = km_customers.fit_predict(customers_df)\n","\n","#Plotando distorção\n","print('Distortion: %.2f' % km_customers.inertia_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ELCFIjbzbfh1"},"source":["Como temos muitos dados com muitas dimensões (features), vamos apenas traçar um histograma do número de clientes por cluster."]},{"cell_type":"code","metadata":{"id":"zleWrlpcbfh2"},"source":["# Quantidade de Clientes por Cluster\n","plt.hist(x=cluster_km_customers, bins=num_clusters)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G5lKHsMfaUv6"},"source":["Agora que os clusters já estão montados, tente interpreta-los:\n","* Quais são as peculiaridades de cada cluster ?\n","* Qual é a caracteristica mais forte de cada um ?\n","\n","Sinta-se a vontade para revisitar a aula de EDA caso necessite de ajuda para montar as análises    "]},{"cell_type":"markdown","metadata":{"id":"0ev_w8HIbfh7"},"source":["<a id=\"modeling_clustering\"></a>\n","### 3.4. Clusterização Hierárquica"]},{"cell_type":"markdown","metadata":{"id":"xNKc31sFbfh8"},"source":["Vamos dar uma olhada em uma abordagem alternativa para clusterização: **Clusterização hierárquica**.\n","\n","Esta técnica é uma altenativa a anterior já que possui um mecanismo diferente para a montagem dos clusters. Esta técnica baseia-se na união das amostras para a montagem do cluster, ou na divisão sequencial do conjunto de todas as amostras para a formação dos clusters (explicaremos melhor abaixo).\n","\n","Uma das vantagens desta técnica é que nos permite traçar dendrogramas (visualizações de um agrupamento hierárquico binário), o que pode ajudar na interpretação dos resultados já que nos permite entender o processo de formação/divisão dos clusters.\n","Outra vantagem útil dessa abordagem hierárquica é que não precisamos especificar o número de clusters antecipadamente."]},{"cell_type":"markdown","metadata":{"id":"21qOBFMEbfh9"},"source":["As duas principais abordagens para agrupamento hierárquico são  o **aglomerativo e  o divisivo**:\n","\n","* **Divisivo**: começa com um cluster que abrange todas as nossas amostras e divide iterativamente o cluster em clusters menores até que cada um contenha apenas uma amostra.\n","\n","* **Aglomerativo**: adota a abordagem oposta, começando com cada amostra como um cluster individual e mesclando os pares mais próximos de clusters até que apenas um cluster permaneça.\n","\n","Nesta seção, vamos nos concentrar no agrupamento aglomerativo, pois é mais comum e útil para obter insights."]},{"cell_type":"markdown","metadata":{"id":"VLvEAsjKbfh9"},"source":["Os dois algoritmos padrão para agrupamento hierárquico aglomerativo são os de **Simple Linkage** e **Complete Linkage**.\n","\n","* **Simple Linkage**: calculamos as distâncias entre os membros mais semelhantes para cada par de clusters e fundimos os dois clusters para os quais a distância entre os membros mais semelhantes é a menor.\n","\n","* **Complete Linkage**: é semelhante à Simple Linkage, mas, em vez de comparar os membros mais semelhantes em cada par de clusters, comparamos os membros mais diferentes para realizar a fusão."]},{"cell_type":"markdown","metadata":{"id":"-86AOeDDbfh-"},"source":["Outros algoritmos comumente usados para agrupamento hierárquico aglomerativo incluem **average linkage** e **Ward's linkage**. No average linkage, mesclamos os pares do cluster com base nas distâncias médias mínimas entre todos os membros do grupo nos dois clusters. No Ward's linkage, os dois clusters que levam ao aumento mínimo do SSE total dentro do cluster são mesclados."]},{"cell_type":"markdown","metadata":{"id":"xq_fi62Vbfh_"},"source":["**Método Aglomerativo:**\n","\n","Este é um procedimento iterativo que pode ser resumido pelo\n","seguintes passos:\n","\n","1. Calcule a matriz de distância de todas as amostras.\n","2. Represente cada ponto de dados como um cluster singleton.\n","3. Mescle os dois clusters mais próximos com base na distância dos membros mais diferentes (distantes).\n","4. Atualize a matriz de distância.\n","5. Repita as etapas 2 a 4 até que um único cluster permaneça.\n","\n","![img](https://dashee87.github.io/images/hierarch.gif)"]},{"cell_type":"markdown","metadata":{"id":"-tg1zJ55bfiA"},"source":["Vamos usar a biblioteca ```scipy``` para traçar um dendograma e visualizar o número de clusters que podem se ajustar melhor aos nossos dados e mostrar```AgglomerativeClustering``` de ``` sklearn``` para computar facilmente os clusters."]},{"cell_type":"code","metadata":{"id":"gYahfbETbfiA"},"source":["#Gerando amostra com 5 clusters\n","X,y = make_blobs(n_samples=500,\n","             n_features=2,\n","             centers=5,\n","             cluster_std=0.5,\n","             shuffle=True,\n","             random_state=0)\n","\n","# Gráfico\n","plt.scatter(X[:,0],\n","            X[:,1],\n","            c=\"black\",\n","            marker='o',\n","            s=50)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vz2DZYJzbfiD"},"source":["# Importando libs\n","from scipy.cluster.hierarchy import dendrogram, linkage  \n","from matplotlib import pyplot as plt\n","\n","# Criação do cluster hierarquico\n","linked = linkage(X, method='complete', metric='euclidean', optimal_ordering=True)\n","\n","# Dendograma\n","plt.figure(figsize=(8, 7))  \n","dendrogram(linked,\n","           orientation='top',\n","           distance_sort='descending',\n","           show_leaf_counts=True)\n","\n","h_line = 3\n","plt.axhline(y=h_line, c='k')\n","plt.show() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vwNw_53nbfiO"},"source":["Este dendrograma nos mostra que temos uma grande diminuição na distância global ao quebrar de 1 para 2 clusters, e também de 2 para 3, e assim por diante até chegarmos de 4 para 5 clusters (aqueles acima da linha horizontal quando ```hline = 3```). Então, a partir de 6 clusters, as distâncias quando adicionamos mais um cluster são muito semelhantes. Isso nos diz, semelhante à regra do cotovelo, que o número de clusters a serem usados aqui é 5.\n","\n","Agora podemos usar ``AgglomerativeClustering`` para separar nossos dados."]},{"cell_type":"code","metadata":{"id":"Ko7UqOkKbfiP"},"source":["# Importando AgglomerativeClustering e StandardScaler\n","from sklearn.cluster import AgglomerativeClustering\n","from sklearn.preprocessing import StandardScaler\n","\n","# Criação do objeto Agglomerative Clustering\n","ac = AgglomerativeClustering(affinity='euclidean',linkage='ward', n_clusters=5)\n","\n","# Fitting e predicting\n","cluster_ac = ac.fit_predict(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eoNxegYhbfiT"},"source":["# Clusters com cores\n","for cluster in range(ac.n_clusters):\n","        plt.scatter(X[cluster_ac==cluster,0],\n","                    X[cluster_ac==cluster,1],\n","                    s=50,\n","                    cmap='Pastel1',\n","                    marker='s',\n","                    label='cluster {}'.format(cluster))\n","\n","plt.legend()\n","plt.grid()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Ny9N_f0bfij"},"source":["\n","### Exercicio 3\n","\n","Vamos tentar usar o clustering hierárquico para nosso conjunto de dados de varejo."]},{"cell_type":"code","metadata":{"id":"ixRmqr5nbfik"},"source":["# Criação do Cluster Hierarquico\n","linked_customers = linkage(customers_df, method='complete', metric='euclidean', optimal_ordering=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"549SMRo4bfi4"},"source":["# Dendograma\n","plt.figure(figsize=(14, 7))  \n","dendrogram(linked_customers,\n","           truncate_mode='lastp',         # usamos o modo truncado para que possamos definir um número máximo p de clusters para mostrar\n","           p=100,                         # definindo p como 100 para tornar o gráfico mais fácil de ler\n","           orientation='top',\n","           distance_sort='descending',\n","           show_leaf_counts=True)\n","\n","plt.show() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fs4aW4G_bfi-"},"source":["#### Parte 1\n","Assim como no KMeans, utilize o gráfico anterior para definir o número de clusters. Em seguida, aplique o ``AgglomerativeClustering`` e plote o histograma dos clientes em cada cluster."]},{"cell_type":"code","metadata":{"id":"I1o4Wv5Ybfi_"},"source":["# Criando o objeto AgglomerativeClustering\n","ac_customers = ___\n","\n","# Fitting e predicting\n","___"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d0bklBoHbfjG"},"source":["# Mostrando a quantidade de clientes por clusters\n","___\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BRUtnFw3aUwx"},"source":["#### Parte 2\n","Agora possuimos novos clusters! Analise e discuta as principais diferenças dos clusters obtidos com o método hierárquico e com o método anterior. "]},{"cell_type":"markdown","metadata":{"id":"Di4Iei5mbfjK"},"source":["<a id=\"soft_clustering_analysis\"></a>\n","## 4. Soft Clustering"]},{"cell_type":"markdown","metadata":{"id":"DfOE9WjsbfjO"},"source":["Nesta seção, apresentaremos a análise de Soft Clustering. Para isso, utilizaremos a base de dados 2018_medias_jogadores.xlsx. Ela é baseada no Cartola FC, que é um jogo que deixa os torcedores muito mais próximos da função de técnico e de diretor de um clube da Série A do Campeonato Brasileiro. O cartoleiro tem como missão escalar seu time a cada rodada do Campeonato Brasileiro, considerando que alguns atletas podem estar lesionados, suspensos ou em condições incertas para entrar em campo na próxima partida. Além disso, cada atleta apresenta diferentes atributos que podem ajudar o cartoleiro a escalar o time da melhor maneira possível, sabendo que cada jogador tem seu preço (medido em cartoletas, moeda oficial do jogo)."]},{"cell_type":"markdown","metadata":{"id":"cPTkyZiIbfjR"},"source":["<a id=\"problem_definition_soft_clustering\"></a>\n","### 4.1. Definição do Problema\n","\n","Suponha que você seja um jogador do Cartola FC e você não escolheu o time da sua semana. Então imagine o seguinte cenário: você está ficando sem dinheiro e por isso não consegue escolher o jogador que está acostumado. Então você se pergunta: _\"Que jogador devo escolher?\"_.\n","\n","Este é um exemplo de problema que pode ser resolvido usando uma técnica de análise de Clusterização. Por exemplo, poderíamos agrupar os jogadores com as mesmas características e, entre aqueles que são semelhantes ao avatar a que está habituado, poderíamos escolher outro com preço inferior."]},{"cell_type":"code","metadata":{"id":"cv5NxrfYbfjS","scrolled":false},"source":["# Importando as Bibliotecas e as base de dados\n","import pandas as pd\n","import numpy as np\n","from sklearn.utils import shuffle\n","\n","df_orig = pd.read_excel('2018_medias_jogadores.xlsx')\n","df_orig = shuffle(df_orig).reset_index(drop = True)\n","df_orig.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kZHXMeXgbfjY"},"source":["<a id=\"initial_analysis_soft_clustering\"></a>\n","### 4.2. Análises Iniciais\n","\n","A principal diferença entre os métodos tradicionais de clustering (o que chamamos de hard clustering) e o soft clustering são os componentes de cada cluster. No Hard Clustering, cada ponto no conjunto de dados pertence a apenas um cluster, enquanto no Soft Clustering cada ponto tem uma probabilidade de estar em um determinado cluster. Em outras palavras, o agrupamento é flexível a ponto de permitir que um item possa existir em \"vários clusters\".\n","\n","No nosso caso, imagine que estamos acostumados a escolher o jogador Lucas Lima, meio-campista do Palmeiras, que vale 8,22 cartoletas. Infelizmente esta semana não podemos pagar este valor. Suponha que usamos um método de agrupamento para descobrir quem são os jogadores semelhantes ao Lucas Lima e seus preços. Podemos ver algumas boas opções mais baratas que Lucas Lima no cluster, então podemos inferir que se pegarmos Everton Ribeiro, por exemplo, há uma grande chance de termos um score semelhante pagando menos cartoletas. \n","\n","![img](https://i.imgur.com/RGbY0Ua.png)\n","\n","Se este problema fosse resolvido usando um método de hard clustering e analisássemos o cluster de Everton Ribeiro, o cluster seria exatamente igual ao de Lucas Lima (mostrado acima). Por outro lado, se escolhermos um método de Soft Clustering, para cada avatar haveria um cluster específico com os mais semelhantes entre eles. Dê uma olhada em um possível soft cluster para Everton Ribeiro e observe que o cluster não é exatamente o mesmo que o de Lucas Lima.\n","\n","![img](https://i.imgur.com/bG2Ta2m.png)\n","\n","O soft clustering nos traz um cluster mais customizado para cada jogador analisado, já que cada jogador tem uma probabilidade de ser semelhante a todos os outros jogadores no jogo de fantasia."]},{"cell_type":"markdown","metadata":{"id":"Lx7hF_2SbfjZ"},"source":["<a id=\"modeling_soft_clustering\"></a>\n","### 4.3. Modelagem"]},{"cell_type":"markdown","metadata":{"id":"sIaLiximbfja"},"source":["Não existe apenas uma implementação da técnica Soft Clustering. Nesta seção, apresentamos uma (das muitas possíveis) implementação baseada no modelo Random Forest. Portanto, para implementá-lo, primeiro precisamos importar todos os pacotes necessários do ```scikit-learn```."]},{"cell_type":"code","metadata":{"id":"K92GkMmSbfja"},"source":["# Importando Bibliotecas\n","from sklearn.model_selection import cross_val_score, GroupKFold\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.utils import shuffle\n","from sklearn.metrics import make_scorer\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.neighbors import KNeighborsRegressor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RQk5Zchvbfjj"},"source":["Além disso, precisamos separar nossas variáveis X e y. Mas, para fazer isso, existem algumas features no conjunto de dados que não devem ser considerados na clusterização. Por exemplo, algumas características categóricas e outras que podem ser extremamente correlacionadas ao preço dos jogadores (relação de causa e efeito)."]},{"cell_type":"code","metadata":{"id":"QgiB5a3-bfjk"},"source":["# Eliminando Variaveis Desnecessárias\n","cols_to_drop = ['player_position','score_no_cleansheets_mean']\n","df = df_orig.drop(cols_to_drop, axis=1)\n","df.head(5)\n","\n","# Removendo possíveis variáveis de causa e efeito e removendo outras categóricas\n","cols_possible_cause_effect = ['score_mean','diff_home_away_s','score_mean_home','score_mean_away']\n","X = df.copy()\n","X.drop(cols_possible_cause_effect, axis=1, inplace=True)\n","X = X.loc[:, 'position_id' : 'DP_mean'].fillna(0)\n","\n","# Obtendo a váriavel Y\n","y = df['price_cartoletas']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4dKVikRmbfjp"},"source":["Agora que os dados estão estruturados corretamente, podemos modelar nosso problema usando a implementação ``scikit-learn`` Random Forest."]},{"cell_type":"code","metadata":{"id":"KMsPYdDjbfjq","scrolled":true},"source":["# Gerando o Modelo de Random Forest\n","rfr = RandomForestRegressor(n_estimators=500, criterion='mse', min_samples_leaf=5)\n","\n","# Fit\n","rfr.fit(X, y)\n","\n","# Obtendo a importância das variaveis\n","importances = pd.Series(index=X.columns, data=rfr.feature_importances_)\n","importances.sort_values(ascending=False, inplace=True)\n","print('Variable importances:\\n',importances)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SlFwlpcQbfjy"},"source":["A implementação parte da ideia de que, para cada jogador, obtemos uma lista de outros jogadores semelhantes ao que foi analisado. Isso pode ser alcançado criando uma matriz dissimilar - esta matriz fornece uma estimativa grosseira da distância entre as amostras com base na proporção de vezes que as amostras terminam no mesmo nó de folha na floresta aleatória (não se preocupe com estes detalhes mais técnicos, explicaremos melhor nas próximas aulas)."]},{"cell_type":"code","metadata":{"id":"k6JYtBP_bfj2"},"source":["# Obtendo Folhas\n","leaves = rfr.apply(X)\n","print('\\nNº Folhas:\\n', leaves, '\\n\\nDimensão das Folhas:', leaves.shape)\n","\n","# Construindo a matriz de dissimilaridade\n","M = leaves.copy()\n","M = OneHotEncoder().fit_transform(M)\n","M = (M * M.transpose()).todense()\n","M = 1 - M / M.max()\n","print('\\nMatriz de Dissimilaridade:\\n', M, '\\n\\nDimensão da Matriz:', M.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4rHtixZvbfj8"},"source":["Observe que, na matriz de dissimilaridade, cada linha *i* e cada coluna *j* representa um jogador. Portanto, o valor (*i*, *j*) é dado pela frequência com que o jogador *i* e o jogador *j* terminaram no mesmo nó folha no Modelo Random Forest (o que significa que eles são similares). Agora, estamos quase prontos para responder ao nosso problema. Mas primeiro, vamos construir nossa estrutura de cluster!"]},{"cell_type":"code","metadata":{"id":"cQ9EGzBFbfj-"},"source":["# Construindo Clusters\n","size_of_cluster = 5\n","neighboors = []\n","distances = []\n","for i in range(len(leaves)):\n","    s = pd.Series(np.array(M[i])[0])\n","    s.drop(i, inplace=True)\n","    # Ordenando os Jogadores\n","    s.sort_values(ascending=True, inplace=True)\n","    neighboors.append([i] + list(s[:size_of_cluster].index))\n","    distances.append([0] + list(s[:size_of_cluster].values))\n","    \n","# Salvando Cluster na estrutura\n","clusters = {}\n","for i in range(len(neighboors)):\n","    L = []\n","    for j in range(len(neighboors[i])):\n","        L.append([neighboors[i][j], y[neighboors[i][j]]])\n","        clusters['C' + str(i)] = L\n","        \n","# Função usada para responder à pergunta: \"Quais são os jogadores semelhantes ao Lucas Lima?\"\n","def getCluster(df, clusters, search_variable, key=None, index=None):\n","    if index == None:\n","        index = df[df[search_variable] == key].index[0]\n","    return df.iloc[[e[0] for e in clusters['C' + str(index)]]]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wvA0GpebbfkC"},"source":["Agora está tudo pronto! Use a célula abaixo para verificar os clusters de cada um dos possíveis jogadores!"]},{"cell_type":"code","metadata":{"id":"yi1c_NxmbfkF"},"source":["getCluster(df=df, clusters=clusters, search_variable='player_slug', key='lucas-lima')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F3gaH65pT3Tw"},"source":["Parabéns! Você conseguiu obter uma lista de jogadores similares a Lucas Lima, o que lhe permite escolher um que tenha um custo menor."]},{"cell_type":"markdown","metadata":{"id":"3wIUlVqWgId1"},"source":["<a id=\"pan\"></a>\n","## Fim da Aula!\n","Com a conclusão desta aula esperamos que você esteja mais familiarizado com os conceitos de Aprendizagem não Supervisionada!\n","Na próxima aula iremos abordar o outro lado da moeda, ou seja, o Aprendizado Supervisionado! Iremos elucidar alguns conceitos e técnicas associados aos problemas de Classificação!\n","\n","Até a próxima aula!!"]}]}